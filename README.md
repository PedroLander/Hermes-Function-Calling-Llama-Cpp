# ğŸ¦™ğŸ”§ Hermes-Function-Calling (CPU Edition) ğŸš€

> **This repository is a modified fork of [NousResearch/Hermes-Function-Calling](https://github.com/NousResearch/Hermes-Function-Calling), adapted (mostly by AI ğŸ¤–) to run efficiently on CPUs using [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)!**

---

## âœ¨ Features

- ğŸ¦™ **Llama 3/2/1 GGUF Model Support** via `llama-cpp-python`
- ğŸ–¥ï¸ **CPU-Only**: No GPU required!
- ğŸ”Œ **OpenAI-Compatible Function Calling** (Tools/Functions)
- ğŸ§© **Composable Prompts** and **Multi-Function Chaining**
- ğŸ› ï¸ **Ready-to-Run Notebooks** for local inference and tool use
- ğŸ“ **Minimal Dependencies** for easy setup

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/Hermes-Function-Calling.git
cd Hermes-Function-Calling
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸ¦™ Model Download

- Download a GGUF model (e.g., Hermes 2 Pro, Llama 3) from [HuggingFace](https://huggingface.co/NousResearch) or [Ollama](https://ollama.com/library).
- Place the `.gguf` file in a known location.

---

## ğŸš€ Usage

### 1ï¸âƒ£ Run Example Notebooks

Open any notebook in `examples/` (e.g., `lllama-cpp-multiple-fn.ipynb`) in JupyterLab or VSCode.

```bash
jupyter lab
```

### 2ï¸âƒ£ Define Your Tools

```python
def get_weather_forecast(location: str) -> dict:
    """Retrieves the weather forecast for a given location"""
    # ...your logic...
```

### 3ï¸âƒ£ Load the Model

```python
import llama_cpp

model = llama_cpp.Llama(
    model_path="path/to/model.gguf",
    n_threads=4,  # Adjust for your CPU
    n_ctx=8192,
)
```

### 4ï¸âƒ£ Compose Messages & Call Functions

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What's the temperature in a random city?"}
]
response = model.create_chat_completion(messages=messages)
print(response)
```

### 5ï¸âƒ£ Parse and Execute Function Calls

- The model will return function calls in a structured format.
- Parse, execute, and feed results back for multi-step reasoning!

---

## ğŸ—‚ï¸ Example Workflow

```mermaid
graph TD
    A[User Query] --> B[Llama Model (llama-cpp-python)]
    B --> C{Function Call?}
    C -- Yes --> D[Parse & Execute Function]
    D --> E[Return Tool Result]
    E --> B
    C -- No --> F[Final Answer]
```

---

## ğŸ› ï¸ Example: Multi-Function Chaining

```python
# Model output:
# <function_calls>
# [
#   {"name": "get_random_city", "output": "random_city"},
#   {"name": "get_weather_forecast", "params": {"location": "random_city"}, "output": "temperature"}
# ]
# </function_calls>
```

---

## ğŸ’¡ Tips

- **CPU Performance:** Increase `n_threads` for faster inference.
- **Context Length:** Adjust `n_ctx` for longer conversations.
- **Model Size:** Use smaller GGUF models for limited RAM.

---

## ğŸ§‘â€ğŸ’» Notebooks Included

- `examples/lllama-cpp-multiple-fn.ipynb` â€” Multi-function tool use
- `examples/ollama_openai_tools.ipynb` â€” Ollama API + OpenAI tools
- `examples/localai_api_fn_calling.ipynb` â€” LocalAI API demo

---

## âš ï¸ Disclaimer

- This is **not** the official NousResearch repo.
- Most modifications were made by AI (GitHub Copilot) to enable CPU-only, llama-cpp-python-based workflows.
- For GPU or HuggingFace Transformers support, see the [original repo](https://github.com/NousResearch/Hermes-Function-Calling).

---

## ğŸ“ License

MIT License (c) 2024 Nous Research

---

## ğŸ¤ Acknowledgements

- [NousResearch](https://github.com/NousResearch) for the original Hermes-Function-Calling
- [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [Ollama](https://ollama.com/)

---

## ğŸŒŸ Happy Function Calling! ğŸŒŸ

